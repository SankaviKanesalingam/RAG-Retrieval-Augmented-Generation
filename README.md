# ğŸ§  RAG with LangChain, Hugging Face & OpenAI | Google Colab

This project demonstrates how to build a **Retrieval-Augmented Generation (RAG)** system using:

- ğŸŒ **LangChain** for document loading and chaining
- ğŸ” **FAISS** for vector search
- ğŸ¤— **Hugging Face Transformers** for the LLM
- ğŸ§  **OpenAI Embeddings** for encoding text
- â˜ï¸ **Google Colab** for execution

---

## ğŸš€ Features

- Fetches and splits web content (e.g., LangSmith docs)
- Converts text chunks into embeddings using OpenAI
- Stores and searches chunks using FAISS vector store
- Uses Hugging Face LLMs to answer questions using retrieved context

---

## ğŸ“š Workflow Overview

1. **Data Loading & Preprocessing**
   - Load website data using LangChain
   - Split content into manageable chunks

2. **Vectorization**
   - Generate embeddings with OpenAI
   - Store chunks in a FAISS vector database

3. **RAG Pipeline**
   - Retrieve relevant chunks for a query
   - Use a Hugging Face model to answer using context

4. **User Interaction**
   - Ask a question
   - Retrieve & respond using LLM + relevant context

---

## ğŸ”§ Installation (on Google Colab)

Paste the following in your Colab notebook:

```bash
!pip install langchain openai huggingface_hub faiss-cpu
